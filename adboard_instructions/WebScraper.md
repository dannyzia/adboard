Project Overview
text
Build an AI-powered web scraper that:
- Scrapes multiple websites from a configurable list
- Uses Crawl4AI + Groq + DeepSeek for intelligent extraction
- Handles different content types with custom schemas
- Supports pagination and dynamic content
- Runs periodically and saves structured data to CSV
- Is modular, scalable, and production-ready
Technical Stack
Web Scraping: Crawl4AI (async)

AI Processing: Groq API + DeepSeek models

Data Handling: Pandas, Pydantic models

Configuration: YAML/JSON files

Scheduling: APScheduler

Error Handling: Comprehensive logging and retries

üóÇÔ∏è Project Structure Instructions
markdown
Create this project structure:

ai_web_scraper/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ websites.yaml          # Website configurations
‚îÇ   ‚îî‚îÄ‚îÄ scraping_rules.yaml    # CSS selectors & rules
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py               # Entry point
‚îÇ   ‚îú‚îÄ‚îÄ scraper/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ multi_site_scraper.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ai_extractor.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data_models.py
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config_loader.py
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ file_handlers.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logger.py
‚îÇ   ‚îî‚îÄ‚îÄ scheduler/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ task_scheduler.py
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ input/                # URL lists, configs
‚îÇ   ‚îî‚îÄ‚îÄ output/               # Scraped results
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .env.example
‚îî‚îÄ‚îÄ README.md
üöÄ Core Implementation Files
1. requirements.txt
txt
crawl4ai>=0.6.0
groq>=0.3.0
pandas>=2.0.0
pydantic>=2.0.0
python-dotenv>=1.0.0
apscheduler>=3.10.0
pyyaml>=6.0
aiofiles>=23.0.0
asyncio>=3.9.0
python-multipart>=0.0.6
beautifulsoup4>=4.12.0
requests>=2.31.0
2. config/websites.yaml
yaml
websites:
  wedding_venues:
    - name: "TheKnot_Venues"
      base_url: "https://www.theknot.com/marketplace/wedding-venues?page={page}"
      paginated: true
      max_pages: 5
      content_type: "wedding_venues"
      css_selectors:
        main: ".listing-card"
        exclude: [".ads", ".sponsored"]
      stop_phrases: ["no results found", "0 matches"]
      rate_limit_delay: 2
      active: true

  news_ads:
    - name: "NYT_Advertisements"
      base_url: "https://www.nytimes.com/section/business"
      paginated: false
      content_type: "news_ads"
      css_selectors:
        main: ".story-wrapper, .ad-container"
      active: true

  real_estate:
    - name: "Zillow_Listings"
      base_url: "https://www.zillow.com/homes/for_sale/?searchQueryState={page}"
      paginated: true
      max_pages: 3
      content_type: "real_estate"
      css_selectors:
        main: ".list-card"
      stop_phrases: ["no homes found"]
      active: true

extraction_schemas:
  wedding_venues:
    fields:
      - name: "venue_name"
        description: "Name of the wedding venue"
        required: true
      - name: "location"
        description: "City and state location"
      - name: "price_range"
        description: "Estimated cost or price range"
      - name: "capacity"
        description: "Guest capacity if mentioned"
      - name: "description"
        description: "One-sentence description generated by AI"
      - name: "contact_info"
        description: "Phone or email if available"
  
  news_ads:
    fields:
      - name: "advertiser"
        description: "Company or brand name"
        required: true
      - name: "product_service"
        description: "What is being advertised"
      - name: "target_audience"
        description: "Who the ad targets"
      - name: "call_to_action"
        description: "What the ad asks viewers to do"
      - name: "estimated_budget"
        description: "Any budget mentions"

  real_estate:
    fields:
      - name: "property_type"
        description: "House, apartment, condo, etc."
        required: true
      - name: "price"
        description: "Listing price"
        required: true
      - name: "location"
        description: "Full address or area"
      - name: "bedrooms"
        description: "Number of bedrooms"
      - name: "bathrooms"
        description: "Number of bathrooms"
      - name: "square_footage"
        description: "Property size"
3. src/config/config_loader.py
python
import yaml
import os
from typing import Dict, Any, List
from pydantic import BaseModel

class WebsiteConfig(BaseModel):
    name: str
    base_url: str
    paginated: bool = False
    max_pages: int = 1
    content_type: str
    css_selectors: Dict[str, Any]
    stop_phrases: List[str] = []
    rate_limit_delay: int = 1
    active: bool = True

class ExtractionSchema(BaseModel):
    fields: List[Dict[str, Any]]

class ConfigLoader:
    def __init__(self, config_path: str = "config/websites.yaml"):
        self.config_path = config_path
        self.websites: List[WebsiteConfig] = []
        self.schemas: Dict[str, ExtractionSchema] = {}
        
    def load_config(self):
        """Load and validate configuration from YAML"""
        with open(self.config_path, 'r') as file:
            config_data = yaml.safe_load(file)
        
        # Load websites
        for category, sites in config_data['websites'].items():
            for site_data in sites:
                self.websites.append(WebsiteConfig(**site_data))
        
        # Load schemas
        for schema_name, schema_data in config_data['extraction_schemas'].items():
            self.schemas[schema_name] = ExtractionSchema(**schema_data)
        
        return self
    
    def get_active_websites(self) -> List[WebsiteConfig]:
        return [site for site in self.websites if site.active]
    
    def get_schema(self, content_type: str) -> ExtractionSchema:
        return self.schemas.get(content_type)
4. src/scraper/data_models.py
python
from pydantic import BaseModel, Field
from typing import Optional, List, Any
from datetime import datetime

class ScrapedItem(BaseModel):
    """Base model for all scraped items"""
    id: Optional[str] = None
    source: str
    source_url: str
    content_type: str
    extracted_data: Dict[str, Any]
    scraped_at: datetime = Field(default_factory=datetime.now)
    confidence_score: Optional[float] = None
    
    class Config:
        arbitrary_types_allowed = True

class ScrapingResult(BaseModel):
    """Result from a scraping session"""
    website: str
    items_found: int
    pages_scraped: int
    success: bool
    error_message: Optional[str] = None
    duration_seconds: float
    items: List[ScrapedItem] = []
5. src/scraper/ai_extractor.py
python
import json
import re
from groq import Groq
from typing import List, Dict, Any
from ..config.config_loader import ExtractionSchema

class AIExtractor:
    def __init__(self, groq_api_key: str, model: str = "deepseek-r1-distill-llama-70b"):
        self.client = Groq(api_key=groq_api_key)
        self.model = model
    
    def build_extraction_prompt(self, content: str, schema: ExtractionSchema, content_type: str) -> str:
        """Build detailed extraction prompt based on schema"""
        
        field_descriptions = []
        for field in schema.fields:
            req = "REQUIRED" if field.get('required', False) else "optional"
            field_descriptions.append(f"- {field['name']} ({req}): {field['description']}")
        
        fields_text = "\n".join(field_descriptions)
        
        prompt = f"""
        EXTRACTION TASK: Extract {content_type} information from the following content.
        
        FIELD SCHEMA:
        {fields_text}
        
        INSTRUCTIONS:
        1. Extract ALL instances you can find in the content
        2. Return ONLY a valid JSON array of objects
        3. Each object must contain the specified fields
        4. For missing optional fields, use null
        5. Generate intelligent descriptions when needed
        6. Be accurate and concise
        
        CONTENT TO ANALYZE:
        {content[:8000]}  # Limit content size
        
        OUTPUT FORMAT:
        Return ONLY JSON array like: [{{"field1": "value1", "field2": "value2"}}]
        """
        
        return prompt
    
    async def extract_structured_data(self, content: str, schema: ExtractionSchema, content_type: str) -> List[Dict[str, Any]]:
        """Extract structured data using AI"""
        
        prompt = self.build_extraction_prompt(content, schema, content_type)
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                max_tokens=4000
            )
            
            result_text = response.choices[0].message.content
            cleaned_json = self.clean_json_response(result_text)
            
            data = json.loads(cleaned_json)
            return data if isinstance(data, list) else []
            
        except Exception as e:
            print(f"AI Extraction failed: {e}")
            return []
    
    def clean_json_response(self, text: str) -> str:
        """Clean and extract JSON from AI response"""
        # Remove markdown code blocks
        text = re.sub(r'```json\s*', '', text)
        text = re.sub(r'\s*```', '', text)
        
        # Find JSON array pattern
        json_match = re.search(r'\[\s*\{.*\}\s*\]', text, re.DOTALL)
        if json_match:
            return json_match.group()
        
        return text.strip()
6. src/scraper/multi_site_scraper.py
python
import asyncio
import aiofiles
import pandas as pd
from crawl4ai import AsyncWebCrawler
from typing import List, Dict, Any
from datetime import datetime
import time

from .data_models import ScrapedItem, ScrapingResult
from .ai_extractor import AIExtractor
from ..config.config_loader import WebsiteConfig, ConfigLoader

class MultiSiteAIScraper:
    def __init__(self, groq_api_key: str, config_loader: ConfigLoader):
        self.groq_api_key = groq_api_key
        self.config_loader = config_loader
        self.ai_extractor = AIExtractor(groq_api_key)
        self.results: List[ScrapingResult] = []
    
    async def run_scraping_session(self) -> List[ScrapingResult]:
        """Main method to run scraping for all active websites"""
        
        active_sites = self.config_loader.get_active_websites()
        tasks = []
        
        # Run sites concurrently with semaphore to limit concurrent requests
        semaphore = asyncio.Semaphore(3)  # Limit to 3 concurrent sites
        
        async def process_with_semaphore(site):
            async with semaphore:
                return await self.process_website(site)
        
        for site in active_sites:
            task = process_with_semaphore(site)
            tasks.append(task)
        
        self.results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Filter out exceptions
        self.results = [r for r in self.results if not isinstance(r, Exception)]
        return self.results
    
    async def process_website(self, site: WebsiteConfig) -> ScrapingResult:
        """Process a single website configuration"""
        
        start_time = time.time()
        scraping_result = ScrapingResult(
            website=site.name,
            items_found=0,
            pages_scraped=0,
            success=False,
            duration_seconds=0
        )
        
        try:
            browser_config = {
                "browser": "chrome",
                "headless": True,
                "window_size": {"width": 1200, "height": 800}
            }
            
            async with AsyncWebCrawler(**browser_config) as crawler:
                if site.paginated:
                    items = await self.scrape_paginated_site(crawler, site)
                else:
                    items = await self.scrape_single_page(crawler, site)
                
                scraping_result.items = items
                scraping_result.items_found = len(items)
                scraping_result.success = True
                
        except Exception as e:
            scraping_result.error_message = str(e)
            print(f"Error processing {site.name}: {e}")
        
        scraping_result.duration_seconds = time.time() - start_time
        return scraping_result
    
    async def scrape_paginated_site(self, crawler, site: WebsiteConfig) -> List[ScrapedItem]:
        """Scrape paginated website"""
        all_items = []
        
        for page in range(1, site.max_pages + 1):
            url = site.base_url.format(page=page)
            print(f"Scraping {site.name} - Page {page}")
            
            # Rate limiting
            await asyncio.sleep(site.rate_limit_delay)
            
            result = await crawler.arun(
                url=url,
                word_count_threshold=10,
                css_selector=site.css_selectors.get('main'),
                exclude_css_selectors=site.css_selectors.get('exclude', []),
                wait_for=5000
            )
            
            if not result or self.should_stop_scraping(result.cleaned_text, site.stop_phrases):
                break
            
            # Extract data with AI
            schema = self.config_loader.get_schema(site.content_type)
            extracted_data = await self.ai_extractor.extract_structured_data(
                result.cleaned_text, schema, site.content_type
            )
            
            # Convert to ScrapedItem objects
            for item_data in extracted_data:
                scraped_item = ScrapedItem(
                    source=site.name,
                    source_url=url,
                    content_type=site.content_type,
                    extracted_data=item_data
                )
                all_items.append(scraped_item)
            
            print(f"Found {len(extracted_data)} items on page {page}")
        
        return all_items
    
    async def scrape_single_page(self, crawler, site: WebsiteConfig) -> List[ScrapedItem]:
        """Scrape single page website"""
        result = await crawler.arun(
            url=site.base_url,
            word_count_threshold=10,
            css_selector=site.css_selectors.get('main'),
            exclude_css_selectors=site.css_selectors.get('exclude', []),
            wait_for=5000
        )
        
        if not result:
            return []
        
        schema = self.config_loader.get_schema(site.content_type)
        extracted_data = await self.ai_extractor.extract_structured_data(
            result.cleaned_text, schema, site.content_type
        )
        
        items = []
        for item_data in extracted_data:
            items.append(ScrapedItem(
                source=site.name,
                source_url=site.base_url,
                content_type=site.content_type,
                extracted_data=item_data
            ))
        
        return items
    
    def should_stop_scraping(self, text: str, stop_phrases: List[str]) -> bool:
        """Check if we should stop scraping based on stop phrases"""
        text_lower = text.lower()
        return any(phrase.lower() in text_lower for phrase in stop_phrases)
    
    def save_results_to_csv(self, filename: str = None):
        """Save all results to CSV"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"data/output/scraping_results_{timestamp}.csv"
        
        all_items = []
        for result in self.results:
            for item in result.items:
                # Flatten the data for CSV
                row_data = {
                    'source': item.source,
                    'source_url': item.source_url,
                    'content_type': item.content_type,
                    'scraped_at': item.scraped_at,
                    **item.extracted_data
                }
                all_items.append(row_data)
        
        if all_items:
            df = pd.DataFrame(all_items)
            df.to_csv(filename, index=False)
            print(f"Saved {len(df)} items to {filename}")
            return filename
        else:
            print("No data to save")
            return None
7. src/scheduler/task_scheduler.py
python
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
import asyncio
from typing import Callable
import logging

class ScrapingScheduler:
    def __init__(self):
        self.scheduler = BackgroundScheduler()
        self.logger = logging.getLogger(__name__)
    
    def add_daily_job(self, task: Callable, hour: int = 2, minute: int = 0):
        """Add daily scraping job"""
        self.scheduler.add_job(
            task,
            trigger=CronTrigger(hour=hour, minute= minute),
            id='daily_scraping',
            name='Daily website scraping',
            replace_existing=True
        )
        self.logger.info(f"Added daily job at {hour:02d}:{minute:02d}")
    
    def add_hourly_job(self, task: Callable, minute: int = 0):
        """Add hourly scraping job"""
        self.scheduler.add_job(
            task,
            trigger=CronTrigger(minute=minute),
            id='hourly_scraping',
            name='Hourly website scraping',
            replace_existing=True
        )
    
    def start(self):
        """Start the scheduler"""
        self.scheduler.start()
        self.logger.info("Scraping scheduler started")
    
    def shutdown(self):
        """Shutdown the scheduler"""
        self.scheduler.shutdown()
        self.logger.info("Scraping scheduler stopped")
8. src/main.py
python
import asyncio
import os
from dotenv import load_dotenv
from scraper.multi_site_scraper import MultiSiteAIScraper
from config.config_loader import ConfigLoader
from scheduler.task_scheduler import ScrapingScheduler
import logging

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraping.log'),
        logging.StreamHandler()
    ]
)

load_dotenv()

class AIScraperApplication:
    def __init__(self):
        self.groq_api_key = os.getenv('GROQ_API_KEY')
        self.config_loader = ConfigLoader()
        self.scraper = None
        self.scheduler = ScrapingScheduler()
    
    async def run_once(self):
        """Run scraping once"""
        print("Starting AI Web Scraper...")
        
        # Load configuration
        self.config_loader.load_config()
        self.scraper = MultiSiteAIScraper(self.groq_api_key, self.config_loader)
        
        # Run scraping
        results = await self.scraper.run_scraping_session()
        
        # Save results
        csv_path = self.scraper.save_results_to_csv()
        
        # Print summary
        total_items = sum(result.items_found for result in results)
        print(f"Scraping completed! Found {total_items} total items.")
        print(f"Results saved to: {csv_path}")
        
        return results
    
    def run_scheduled(self):
        """Run with scheduling"""
        self.scheduler.add_daily_job(
            lambda: asyncio.run(self.run_once()),
            hour=2,  # 2 AM daily
            minute=0
        )
        self.scheduler.start()
        
        print("Scheduler started. Press Ctrl+C to stop.")
        try:
            # Keep the script running
            while True:
                pass
        except KeyboardInterrupt:
            self.scheduler.shutdown()

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='AI Web Scraper')
    parser.add_argument('--scheduled', action='store_true', help='Run with scheduling')
    
    args = parser.parse_args()
    
    app = AIScraperApplication()
    
    if args.scheduled:
        app.run_scheduled()
    else:
        asyncio.run(app.run_once())
9. .env.example
env
GROQ_API_KEY=your_groq_api_key_here
LOG_LEVEL=INFO
MAX_CONCURRENT_SITES=3
DEFAULT_RATE_LIMIT_DELAY=1
10. README.md
markdown
# AI-Powered Multi-Site Web Scraper

Intelligent web scraper using Crawl4AI, Groq, and DeepSeek models to extract structured data from multiple websites.

## Features
- ü§ñ AI-powered data extraction with custom schemas
- üåê Multi-website support with configuration
- ‚è∞ Scheduled scraping with APScheduler
- üìä Structured data export to CSV
- üöÄ Async processing for performance
- üîß Configurable via YAML files

## Quick Start

1. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
Configure environment:

bash
cp .env.example .env
# Add your Groq API key to .env
Configure websites:
Edit config/websites.yaml with your target sites

Run once:

bash
python src/main.py
Run with scheduling:

bash
python src/main.py --scheduled
Configuration
Add websites in config/websites.yaml

Define extraction schemas for different content types

Set up CSS selectors for each site

Configure scheduling in src/main.py

text

---

## üéØ Instructions for GitHub Copilot

**Copy this entire structure and paste it to GitHub Copilot with this prompt:**
I'm building an AI-powered multi-website scraper. Here's my complete project structure and code. Please help me:

IMPLEMENT MISSING PARTS: Complete any incomplete methods or classes

ADD ERROR HANDLING: Implement robust error handling and retries

ENHANCE FEATURES: Add rate limiting, proxy support, and better logging

OPTIMIZE PERFORMANCE: Improve async processing and memory usage

ADD TESTS: Create unit tests for key components

DOCKER SUPPORT: Add Dockerfile and docker-compose for deployment

MONITORING: Add health checks and monitoring endpoints

DOCUMENTATION: Improve docstrings and user documentation

Please generate the complete code for any missing parts and suggest improvements for the existing codebase.